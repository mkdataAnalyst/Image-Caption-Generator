import string
import pandas as pd
from nltk.tokenize import word_tokenize

def clean_caption(caption):
    caption = caption.lower()
    caption = caption.translate(str.maketrans('', '', string.punctuation))
    return '<start> ' + caption + ' <end>'

# Load captions
df = pd.read_csv('../data/raw/Flickr8k.token.txt', sep='\t', header=None, names=['image', 'caption'])
df['image'] = df['image'].apply(lambda x: x.split('#')[0])
df['caption'] = df['caption'].apply(clean_caption)

# Save cleaned captions
df.to_csv('../data/processed/cleaned_captions.csv', index=False)
print(df.head())# Preprocessing Notebook
